{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 785)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 785)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 785)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 785)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_MNIST_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_MNIST_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.288668\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(785, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *at the beginning of training when weights are initialized to random values loses should be close to -ln(N), where N - number of classes.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.000000 analytic: -0.000000, relative error: 4.741009e-05\n",
      "numerical: 0.167678 analytic: 0.167677, relative error: 1.093214e-06\n",
      "numerical: 0.175912 analytic: 0.175912, relative error: 2.267056e-07\n",
      "numerical: 0.033233 analytic: 0.033233, relative error: 3.930266e-08\n",
      "numerical: -1.042421 analytic: -1.042423, relative error: 6.721005e-07\n",
      "numerical: -0.000092 analytic: -0.000092, relative error: 1.174914e-07\n",
      "numerical: 6.612874 analytic: 6.612873, relative error: 9.952316e-08\n",
      "numerical: 0.000023 analytic: 0.000023, relative error: 1.070104e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vova/Dropbox/nn/cs231/assignment1/cs231n/gradient_check.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  (abs(grad_numerical) + abs(grad_analytic)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.169610 analytic: 0.169610, relative error: 2.541533e-07\n",
      "numerical: 1.821075 analytic: 1.821074, relative error: 2.136304e-07\n",
      "numerical: 1.533358 analytic: 1.533357, relative error: 2.536057e-07\n",
      "numerical: -3.216799 analytic: -3.216799, relative error: 7.411808e-08\n",
      "numerical: -0.011765 analytic: -0.011765, relative error: 6.398488e-09\n",
      "numerical: -0.000911 analytic: -0.000911, relative error: 1.161280e-09\n",
      "numerical: -0.469776 analytic: -0.469776, relative error: 3.696443e-08\n",
      "numerical: -0.007888 analytic: -0.007888, relative error: 5.080867e-09\n",
      "numerical: 0.072765 analytic: 0.072765, relative error: 1.086658e-07\n",
      "numerical: -0.002277 analytic: -0.002277, relative error: 1.830457e-08\n",
      "numerical: 0.051965 analytic: 0.051965, relative error: 8.373774e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.288668e+00 computed in 0.073321s\n",
      "vectorized loss: 2.288668e+00 computed in 0.010593s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 191.789261\n",
      "iteration 100 / 1000: loss 70.967051\n",
      "iteration 200 / 1000: loss 26.977489\n",
      "iteration 300 / 1000: loss 11.052101\n",
      "iteration 400 / 1000: loss 5.139214\n",
      "iteration 500 / 1000: loss 3.029332\n",
      "iteration 600 / 1000: loss 2.292363\n",
      "iteration 700 / 1000: loss 2.017704\n",
      "iteration 800 / 1000: loss 1.860442\n",
      "iteration 900 / 1000: loss 1.853375\n",
      "That took 1.133903s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cs231n.classifiers import Softmax\n",
    "softmax = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=1000, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 159.044690\n",
      "iteration 100 / 1500: loss 71.861933\n",
      "iteration 200 / 1500: loss 32.876247\n",
      "iteration 300 / 1500: loss 15.663947\n",
      "iteration 400 / 1500: loss 7.969064\n",
      "iteration 500 / 1500: loss 4.451654\n",
      "iteration 600 / 1500: loss 3.028184\n",
      "iteration 700 / 1500: loss 2.296119\n",
      "iteration 800 / 1500: loss 1.995131\n",
      "iteration 900 / 1500: loss 1.901297\n",
      "iteration 1000 / 1500: loss 1.758180\n",
      "iteration 1100 / 1500: loss 1.741139\n",
      "iteration 1200 / 1500: loss 1.790292\n",
      "iteration 1300 / 1500: loss 1.737577\n",
      "iteration 1400 / 1500: loss 1.766259\n",
      "iteration 0 / 1500: loss 197.481577\n",
      "iteration 100 / 1500: loss 73.188266\n",
      "iteration 200 / 1500: loss 27.790723\n",
      "iteration 300 / 1500: loss 11.311510\n",
      "iteration 400 / 1500: loss 5.274629\n",
      "iteration 500 / 1500: loss 3.108906\n",
      "iteration 600 / 1500: loss 2.319264\n",
      "iteration 700 / 1500: loss 1.966877\n",
      "iteration 800 / 1500: loss 1.815951\n",
      "iteration 900 / 1500: loss 1.819901\n",
      "iteration 1000 / 1500: loss 1.839906\n",
      "iteration 1100 / 1500: loss 1.812627\n",
      "iteration 1200 / 1500: loss 1.860931\n",
      "iteration 1300 / 1500: loss 1.790859\n",
      "iteration 1400 / 1500: loss 1.838770\n",
      "iteration 0 / 1500: loss 272.195556\n",
      "iteration 100 / 1500: loss 67.972966\n",
      "iteration 200 / 1500: loss 17.956702\n",
      "iteration 300 / 1500: loss 5.845978\n",
      "iteration 400 / 1500: loss 2.837347\n",
      "iteration 500 / 1500: loss 2.155150\n",
      "iteration 600 / 1500: loss 1.993667\n",
      "iteration 700 / 1500: loss 1.956174\n",
      "iteration 800 / 1500: loss 1.919663\n",
      "iteration 900 / 1500: loss 1.942471\n",
      "iteration 1000 / 1500: loss 1.955824\n",
      "iteration 1100 / 1500: loss 1.894371\n",
      "iteration 1200 / 1500: loss 1.918738\n",
      "iteration 1300 / 1500: loss 1.920111\n",
      "iteration 1400 / 1500: loss 1.904232\n",
      "iteration 0 / 1500: loss 397.931101\n",
      "iteration 100 / 1500: loss 54.768902\n",
      "iteration 200 / 1500: loss 9.036323\n",
      "iteration 300 / 1500: loss 2.966541\n",
      "iteration 400 / 1500: loss 2.103366\n",
      "iteration 500 / 1500: loss 2.013641\n",
      "iteration 600 / 1500: loss 2.028551\n",
      "iteration 700 / 1500: loss 2.052916\n",
      "iteration 800 / 1500: loss 1.981833\n",
      "iteration 900 / 1500: loss 1.979051\n",
      "iteration 1000 / 1500: loss 2.003585\n",
      "iteration 1100 / 1500: loss 2.006757\n",
      "iteration 1200 / 1500: loss 2.032536\n",
      "iteration 1300 / 1500: loss 2.005927\n",
      "iteration 1400 / 1500: loss 2.038512\n",
      "iteration 0 / 1500: loss 158.560409\n",
      "iteration 100 / 1500: loss 32.779825\n",
      "iteration 200 / 1500: loss 7.885459\n",
      "iteration 300 / 1500: loss 3.031398\n",
      "iteration 400 / 1500: loss 2.034045\n",
      "iteration 500 / 1500: loss 1.794280\n",
      "iteration 600 / 1500: loss 1.774474\n",
      "iteration 700 / 1500: loss 1.769539\n",
      "iteration 800 / 1500: loss 1.704886\n",
      "iteration 900 / 1500: loss 1.680206\n",
      "iteration 1000 / 1500: loss 1.796943\n",
      "iteration 1100 / 1500: loss 1.807844\n",
      "iteration 1200 / 1500: loss 1.760732\n",
      "iteration 1300 / 1500: loss 1.732075\n",
      "iteration 1400 / 1500: loss 1.751954\n",
      "iteration 0 / 1500: loss 200.797049\n",
      "iteration 100 / 1500: loss 28.032416\n",
      "iteration 200 / 1500: loss 5.226747\n",
      "iteration 300 / 1500: loss 2.274874\n",
      "iteration 400 / 1500: loss 1.899502\n",
      "iteration 500 / 1500: loss 1.821582\n",
      "iteration 600 / 1500: loss 1.853675\n",
      "iteration 700 / 1500: loss 1.813326\n",
      "iteration 800 / 1500: loss 1.810820\n",
      "iteration 900 / 1500: loss 1.848286\n",
      "iteration 1000 / 1500: loss 1.782552\n",
      "iteration 1100 / 1500: loss 1.819944\n",
      "iteration 1200 / 1500: loss 1.896204\n",
      "iteration 1300 / 1500: loss 1.818713\n",
      "iteration 1400 / 1500: loss 1.874932\n",
      "iteration 0 / 1500: loss 284.193985\n",
      "iteration 100 / 1500: loss 18.550348\n",
      "iteration 200 / 1500: loss 2.891869\n",
      "iteration 300 / 1500: loss 2.016696\n",
      "iteration 400 / 1500: loss 1.946412\n",
      "iteration 500 / 1500: loss 1.867217\n",
      "iteration 600 / 1500: loss 1.948900\n",
      "iteration 700 / 1500: loss 1.886425\n",
      "iteration 800 / 1500: loss 1.909778\n",
      "iteration 900 / 1500: loss 1.883655\n",
      "iteration 1000 / 1500: loss 1.929210\n",
      "iteration 1100 / 1500: loss 1.955198\n",
      "iteration 1200 / 1500: loss 1.907332\n",
      "iteration 1300 / 1500: loss 1.905057\n",
      "iteration 1400 / 1500: loss 1.932611\n",
      "iteration 0 / 1500: loss 385.286018\n",
      "iteration 100 / 1500: loss 8.674303\n",
      "iteration 200 / 1500: loss 2.163615\n",
      "iteration 300 / 1500: loss 2.041076\n",
      "iteration 400 / 1500: loss 2.021784\n",
      "iteration 500 / 1500: loss 2.005572\n",
      "iteration 600 / 1500: loss 2.040239\n",
      "iteration 700 / 1500: loss 1.977566\n",
      "iteration 800 / 1500: loss 2.028976\n",
      "iteration 900 / 1500: loss 1.997293\n",
      "iteration 1000 / 1500: loss 1.967910\n",
      "iteration 1100 / 1500: loss 2.003608\n",
      "iteration 1200 / 1500: loss 2.016174\n",
      "iteration 1300 / 1500: loss 2.002699\n",
      "iteration 1400 / 1500: loss 2.012868\n",
      "iteration 0 / 1500: loss 160.541284\n",
      "iteration 100 / 1500: loss 15.710969\n",
      "iteration 200 / 1500: loss 2.991443\n",
      "iteration 300 / 1500: loss 1.918712\n",
      "iteration 400 / 1500: loss 1.761869\n",
      "iteration 500 / 1500: loss 1.791581\n",
      "iteration 600 / 1500: loss 1.740983\n",
      "iteration 700 / 1500: loss 1.741936\n",
      "iteration 800 / 1500: loss 1.721642\n",
      "iteration 900 / 1500: loss 1.750621\n",
      "iteration 1000 / 1500: loss 1.762829\n",
      "iteration 1100 / 1500: loss 1.660894\n",
      "iteration 1200 / 1500: loss 1.698117\n",
      "iteration 1300 / 1500: loss 1.742842\n",
      "iteration 1400 / 1500: loss 1.722499\n",
      "iteration 0 / 1500: loss 195.243342\n",
      "iteration 100 / 1500: loss 11.044643\n",
      "iteration 200 / 1500: loss 2.301228\n",
      "iteration 300 / 1500: loss 1.858164\n",
      "iteration 400 / 1500: loss 1.808952\n",
      "iteration 500 / 1500: loss 1.819756\n",
      "iteration 600 / 1500: loss 1.804373\n",
      "iteration 700 / 1500: loss 1.798997\n",
      "iteration 800 / 1500: loss 1.826852\n",
      "iteration 900 / 1500: loss 1.783898\n",
      "iteration 1000 / 1500: loss 1.766614\n",
      "iteration 1100 / 1500: loss 1.883781\n",
      "iteration 1200 / 1500: loss 1.796461\n",
      "iteration 1300 / 1500: loss 1.835719\n",
      "iteration 1400 / 1500: loss 1.805416\n",
      "iteration 0 / 1500: loss 281.311315\n",
      "iteration 100 / 1500: loss 5.883844\n",
      "iteration 200 / 1500: loss 1.931723\n",
      "iteration 300 / 1500: loss 1.949721\n",
      "iteration 400 / 1500: loss 1.916317\n",
      "iteration 500 / 1500: loss 1.926521\n",
      "iteration 600 / 1500: loss 1.941456\n",
      "iteration 700 / 1500: loss 1.886139\n",
      "iteration 800 / 1500: loss 1.877521\n",
      "iteration 900 / 1500: loss 1.919100\n",
      "iteration 1000 / 1500: loss 1.949671\n",
      "iteration 1100 / 1500: loss 1.941776\n",
      "iteration 1200 / 1500: loss 1.963395\n",
      "iteration 1300 / 1500: loss 1.938003\n",
      "iteration 1400 / 1500: loss 1.884908\n",
      "iteration 0 / 1500: loss 394.963230\n",
      "iteration 100 / 1500: loss 2.893981\n",
      "iteration 200 / 1500: loss 2.018816\n",
      "iteration 300 / 1500: loss 2.017295\n",
      "iteration 400 / 1500: loss 2.068552\n",
      "iteration 500 / 1500: loss 2.039777\n",
      "iteration 600 / 1500: loss 1.986870\n",
      "iteration 700 / 1500: loss 2.036169\n",
      "iteration 800 / 1500: loss 1.975989\n",
      "iteration 900 / 1500: loss 2.010893\n",
      "iteration 1000 / 1500: loss 2.003742\n",
      "iteration 1100 / 1500: loss 2.031347\n",
      "iteration 1200 / 1500: loss 2.011616\n",
      "iteration 1300 / 1500: loss 2.064246\n",
      "iteration 1400 / 1500: loss 1.988192\n",
      "iteration 0 / 1500: loss 159.638523\n",
      "iteration 100 / 1500: loss 4.418533\n",
      "iteration 200 / 1500: loss 1.764138\n",
      "iteration 300 / 1500: loss 1.777425\n",
      "iteration 400 / 1500: loss 1.801227\n",
      "iteration 500 / 1500: loss 1.714158\n",
      "iteration 600 / 1500: loss 1.825356\n",
      "iteration 700 / 1500: loss 1.742888\n",
      "iteration 800 / 1500: loss 1.800735\n",
      "iteration 900 / 1500: loss 1.752235\n",
      "iteration 1000 / 1500: loss 1.735670\n",
      "iteration 1100 / 1500: loss 1.767460\n",
      "iteration 1200 / 1500: loss 1.705396\n",
      "iteration 1300 / 1500: loss 1.820280\n",
      "iteration 1400 / 1500: loss 1.784681\n",
      "iteration 0 / 1500: loss 199.992920\n",
      "iteration 100 / 1500: loss 3.045125\n",
      "iteration 200 / 1500: loss 1.864050\n",
      "iteration 300 / 1500: loss 1.842039\n",
      "iteration 400 / 1500: loss 1.828540\n",
      "iteration 500 / 1500: loss 1.800578\n",
      "iteration 600 / 1500: loss 1.812781\n",
      "iteration 700 / 1500: loss 1.859762\n",
      "iteration 800 / 1500: loss 1.838227\n",
      "iteration 900 / 1500: loss 1.825456\n",
      "iteration 1000 / 1500: loss 1.829859\n",
      "iteration 1100 / 1500: loss 1.856050\n",
      "iteration 1200 / 1500: loss 1.782318\n",
      "iteration 1300 / 1500: loss 1.785095\n",
      "iteration 1400 / 1500: loss 1.846665\n",
      "iteration 0 / 1500: loss 277.057494\n",
      "iteration 100 / 1500: loss 2.169529\n",
      "iteration 200 / 1500: loss 1.922620\n",
      "iteration 300 / 1500: loss 1.893164\n",
      "iteration 400 / 1500: loss 1.956064\n",
      "iteration 500 / 1500: loss 1.948052\n",
      "iteration 600 / 1500: loss 1.951268\n",
      "iteration 700 / 1500: loss 1.962273\n",
      "iteration 800 / 1500: loss 1.932494\n",
      "iteration 900 / 1500: loss 1.925641\n",
      "iteration 1000 / 1500: loss 1.906797\n",
      "iteration 1100 / 1500: loss 1.910431\n",
      "iteration 1200 / 1500: loss 1.882454\n",
      "iteration 1300 / 1500: loss 1.914472\n",
      "iteration 1400 / 1500: loss 1.868270\n",
      "iteration 0 / 1500: loss 406.355422\n",
      "iteration 100 / 1500: loss 2.040081\n",
      "iteration 200 / 1500: loss 2.036989\n",
      "iteration 300 / 1500: loss 1.999807\n",
      "iteration 400 / 1500: loss 2.046256\n",
      "iteration 500 / 1500: loss 2.058843\n",
      "iteration 600 / 1500: loss 2.012938\n",
      "iteration 700 / 1500: loss 2.031076\n",
      "iteration 800 / 1500: loss 2.042317\n",
      "iteration 900 / 1500: loss 2.043181\n",
      "iteration 1000 / 1500: loss 2.020822\n",
      "iteration 1100 / 1500: loss 1.991990\n",
      "iteration 1200 / 1500: loss 2.013837\n",
      "iteration 1300 / 1500: loss 2.012329\n",
      "iteration 1400 / 1500: loss 2.008647\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.784204 val accuracy: 0.752000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.775714 val accuracy: 0.738000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.762714 val accuracy: 0.719000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.754673 val accuracy: 0.712000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.784633 val accuracy: 0.749000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.773653 val accuracy: 0.740000\n",
      "lr 2.000000e-07 reg 3.500000e+04 train accuracy: 0.768449 val accuracy: 0.727000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.754245 val accuracy: 0.710000\n",
      "lr 3.000000e-07 reg 2.000000e+04 train accuracy: 0.778980 val accuracy: 0.748000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.770939 val accuracy: 0.740000\n",
      "lr 3.000000e-07 reg 3.500000e+04 train accuracy: 0.764857 val accuracy: 0.719000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.749735 val accuracy: 0.709000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.776878 val accuracy: 0.740000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.775041 val accuracy: 0.739000\n",
      "lr 5.000000e-07 reg 3.500000e+04 train accuracy: 0.760367 val accuracy: 0.726000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.762408 val accuracy: 0.722000\n",
      "best validation accuracy achieved during cross-validation: 0.752000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 2e-7, 3e-7, 5e-7]\n",
    "regularization_strengths = [2e4, 2.5e4, 3.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=lr,\n",
    "                      reg=reg, num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        \n",
    "        y_train_acc = (np.mean(y_train == y_train_pred))\n",
    "        y_val_acc = (np.mean(y_val == y_val_pred))\n",
    "        \n",
    "        results[(lr, reg)] = (y_train_acc, y_val_acc)\n",
    "        if y_val_acc > best_val:\n",
    "            best_val = y_val_acc \n",
    "            best_softmax = softmax\n",
    "        \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.766000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *True*\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$ *SVM loss only takes into account datapoints that are above a certain threshold it's not the case with softmax.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(28, 28, 1, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = range(0, 10)\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
